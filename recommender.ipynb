{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b60d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINK TO AMAZON DATASET: https://nijianmo.github.io/amazon/index.html#sample-metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4861127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882afdbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert ReviewerID to unique integers\n",
    "#new_reviewerID = random.sample(range(0, 4607047), 4607047)\n",
    "\n",
    "#new_reviewerID = spark.createDataFrame(new_reviewerID, IntegerType()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b0eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, DoubleType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84039f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('/home/luca/Downloads/ratings_Movies_and_TV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0d83f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0324a364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,StringType,true),StructField(_c1,StringType,true),StructField(_c2,StringType,true),StructField(_c3,StringType,true)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce986af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='A17W587EH23J0Q', _c1='B00LT1JHLW', _c2='5.0', _c3='1405641600'),\n",
       " Row(_c0='A3E4Q2YOYCKXON', _c1='B00LT1JHLW', _c2='5.0', _c3='1405987200'),\n",
       " Row(_c0='A1U1UNV1RLCKRL', _c1='B00LT1JHLW', _c2='3.0', _c3='1406073600'),\n",
       " Row(_c0='A14THKG1X8861X', _c1='B00LT1JHLW', _c2='5.0', _c3='1405555200'),\n",
       " Row(_c0='A3DE438TF1A958', _c1='B00LT1JHLW', _c2='5.0', _c3='1405728000'),\n",
       " Row(_c0='AHCV1RTGY3PJ8', _c1='B00LT1JHLW', _c2='5.0', _c3='1405641600'),\n",
       " Row(_c0='A2RWCXDMANY0LW', _c1='B00LT1JHLW', _c2='5.0', _c3='1405987200'),\n",
       " Row(_c0='A3V9PIFRME2XCW', _c1='B00LT1JHLW', _c2='5.0', _c3='1405900800'),\n",
       " Row(_c0='A3ROPC55BE2OM9', _c1='B00LT1JHLW', _c2='5.0', _c3='1405728000'),\n",
       " Row(_c0='A2ARBNMH5Q5YM1', _c1='B00LVGP8EA', _c2='5.0', _c3='1405641600')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657c1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing column names - https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark\n",
    "\n",
    "df = df.selectExpr(\"_c0 as ReviewerID\", \"_c1 as ProductID\", \"_c2 as Rating\", \"_c3 as unixReviewTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b0b3cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+--------------+\n",
      "|    ReviewerID| ProductID|Rating|unixReviewTime|\n",
      "+--------------+----------+------+--------------+\n",
      "|A3R5OBKS7OM2IR|0000143502|   5.0|    1358380800|\n",
      "|A3R5OBKS7OM2IR|0000143529|   5.0|    1380672000|\n",
      "| AH3QC2PC1VTGP|0000143561|   2.0|    1216252800|\n",
      "|A3LKP6WPMP9UKX|0000143588|   5.0|    1236902400|\n",
      "| AVIY68KEPQ5ZD|0000143588|   5.0|    1232236800|\n",
      "|A1CV1WROP5KTTW|0000589012|   5.0|    1309651200|\n",
      "| AP57WZ2X4G0AA|0000589012|   2.0|    1366675200|\n",
      "|A3NMBJ2LCRCATT|0000589012|   5.0|    1393804800|\n",
      "| A5Y15SAOMX6XA|0000589012|   2.0|    1307404800|\n",
      "|A3P671HJ32TCSF|0000589012|   5.0|    1393718400|\n",
      "|A3VCKTRD24BG7K|0000589012|   5.0|    1378425600|\n",
      "| ANF0AGIV0JCH2|0000589012|   5.0|    1308182400|\n",
      "|A3LDEBLV6MVUBE|0000589012|   5.0|    1208995200|\n",
      "|A1R2XZWQ6NM5M1|0000589012|   5.0|    1224979200|\n",
      "|A36L1XGA5AQIJY|0000589012|   1.0|    1393113600|\n",
      "|A2HWI21H23GDS4|0000589012|   4.0|    1338681600|\n",
      "|A1DNYFL3RSXRMO|0000589012|   5.0|    1208908800|\n",
      "|A39VF226GBM1JH|0000589012|   1.0|    1218412800|\n",
      "| ASB0E2O2FLNA7|0000589012|   5.0|    1322956800|\n",
      "|A19E15Y9V09CVJ|0000589012|   5.0|    1224806400|\n",
      "+--------------+----------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Items of interest are ReviewerID and ProductID\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0972a551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4607047"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MUST FIT INTEGER RANGE: -2147483648 to 2147483647 - https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8c3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.take(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90df6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4215022d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ReviewerID', 'string'),\n",
       " ('ProductID', 'string'),\n",
       " ('Rating', 'string'),\n",
       " ('unixReviewTime', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "441a72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ReviewerID\", df[\"ReviewerID\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"ProductID\", df[\"ProductID\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Rating\", df[\"Rating\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"unixReviewTime\", df[\"unixReviewTime\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65096648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "|      null|   143502|     5|    1358380800|\n",
      "|      null|   143529|     5|    1380672000|\n",
      "|      null|   143561|     2|    1216252800|\n",
      "|      null|   143588|     5|    1236902400|\n",
      "|      null|   143588|     5|    1232236800|\n",
      "|      null|   589012|     5|    1309651200|\n",
      "|      null|   589012|     2|    1366675200|\n",
      "|      null|   589012|     5|    1393804800|\n",
      "|      null|   589012|     2|    1307404800|\n",
      "|      null|   589012|     5|    1393718400|\n",
      "|      null|   589012|     5|    1378425600|\n",
      "|      null|   589012|     5|    1308182400|\n",
      "|      null|   589012|     5|    1208995200|\n",
      "|      null|   589012|     5|    1224979200|\n",
      "|      null|   589012|     1|    1393113600|\n",
      "|      null|   589012|     4|    1338681600|\n",
      "|      null|   589012|     5|    1208908800|\n",
      "|      null|   589012|     1|    1218412800|\n",
      "|      null|   589012|     5|    1322956800|\n",
      "|      null|   589012|     5|    1224806400|\n",
      "+----------+---------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Items of interest are ReviewerID and ProductID\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119ef7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Taken from: https://stackoverflow.com/questions/44153575/fill-na-with-random-numbers-in-pyspark\n",
    "new_df = df.withColumn('ReviewerID', F.coalesce(F.col('ReviewerID'), (F.round(F.rand()*100000)))).collect()\n",
    "new_df = spark.createDataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a10e8b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "|   20897.0|   143502|     5|    1358380800|\n",
      "|    7518.0|   143529|     5|    1380672000|\n",
      "|   53908.0|   143561|     2|    1216252800|\n",
      "|   81400.0|   143588|     5|    1236902400|\n",
      "|   11893.0|   143588|     5|    1232236800|\n",
      "|   25607.0|   589012|     5|    1309651200|\n",
      "|   79693.0|   589012|     2|    1366675200|\n",
      "|    6297.0|   589012|     5|    1393804800|\n",
      "|   79307.0|   589012|     2|    1307404800|\n",
      "|    3650.0|   589012|     5|    1393718400|\n",
      "|   72012.0|   589012|     5|    1378425600|\n",
      "|   85204.0|   589012|     5|    1308182400|\n",
      "|   60006.0|   589012|     5|    1208995200|\n",
      "|   19893.0|   589012|     5|    1224979200|\n",
      "|   21909.0|   589012|     1|    1393113600|\n",
      "|   13983.0|   589012|     4|    1338681600|\n",
      "|   97826.0|   589012|     5|    1208908800|\n",
      "|   48180.0|   589012|     1|    1218412800|\n",
      "|    3207.0|   589012|     5|    1322956800|\n",
      "|   14854.0|   589012|     5|    1224806400|\n",
      "+----------+---------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef9f1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57cd54c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ReviewerID', 'double'),\n",
       " ('ProductID', 'bigint'),\n",
       " ('Rating', 'bigint'),\n",
       " ('unixReviewTime', 'bigint')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e11d56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn(\"ReviewerID\", new_df[\"ReviewerID\"].cast(IntegerType()))\n",
    "new_df = new_df.withColumn(\"ProductID\", new_df[\"ProductID\"].cast(IntegerType()))\n",
    "new_df = new_df.withColumn(\"Rating\", new_df[\"Rating\"].cast(DoubleType()))\n",
    "new_df = new_df.withColumn(\"unixReviewTime\", new_df[\"unixReviewTime\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42408b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ReviewerID', 'int'),\n",
       " ('ProductID', 'int'),\n",
       " ('Rating', 'double'),\n",
       " ('unixReviewTime', 'int')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b38d833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "|     20897|   143502|   5.0|    1358380800|\n",
      "|      7518|   143529|   5.0|    1380672000|\n",
      "|     53908|   143561|   2.0|    1216252800|\n",
      "|     81400|   143588|   5.0|    1236902400|\n",
      "|     11893|   143588|   5.0|    1232236800|\n",
      "|     25607|   589012|   5.0|    1309651200|\n",
      "|     79693|   589012|   2.0|    1366675200|\n",
      "|      6297|   589012|   5.0|    1393804800|\n",
      "|     79307|   589012|   2.0|    1307404800|\n",
      "|      3650|   589012|   5.0|    1393718400|\n",
      "|     72012|   589012|   5.0|    1378425600|\n",
      "|     85204|   589012|   5.0|    1308182400|\n",
      "|     60006|   589012|   5.0|    1208995200|\n",
      "|     19893|   589012|   5.0|    1224979200|\n",
      "|     21909|   589012|   1.0|    1393113600|\n",
      "|     13983|   589012|   4.0|    1338681600|\n",
      "|     97826|   589012|   5.0|    1208908800|\n",
      "|     48180|   589012|   1.0|    1218412800|\n",
      "|      3207|   589012|   5.0|    1322956800|\n",
      "|     14854|   589012|   5.0|    1224806400|\n",
      "+----------+---------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e97c9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = new_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b97def2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o112.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 24) (luca executor driver): org.apache.spark.SparkException: Failed to execute user defined function(ALSModelParams$$Lambda$3108/0x000000084125b040: (int) => int)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1449)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: ALS only supports values in Integer range for columns ReviewerID and ProductID. Value null was not numeric.\n\tat org.apache.spark.ml.recommendation.ALSModelParams.$anonfun$checkedCast$1(ALS.scala:104)\n\tat org.apache.spark.ml.recommendation.ALSModelParams.$anonfun$checkedCast$1$adapted(ALS.scala:89)\n\t... 33 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDD.$anonfun$isEmpty$1(RDD.scala:1557)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.isEmpty(RDD.scala:1557)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:947)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:709)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(ALSModelParams$$Lambda$3108/0x000000084125b040: (int) => int)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1449)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: ALS only supports values in Integer range for columns ReviewerID and ProductID. Value null was not numeric.\n\tat org.apache.spark.ml.recommendation.ALSModelParams.$anonfun$checkedCast$1(ALS.scala:104)\n\tat org.apache.spark.ml.recommendation.ALSModelParams.$anonfun$checkedCast$1$adapted(ALS.scala:89)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-976f07881cd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m als = ALS(maxIter=5, regParam=0.01, userCol=\"ReviewerID\", itemCol=\"ProductID\", ratingCol=\"Rating\",\n\u001b[1;32m      2\u001b[0m           coldStartStrategy=\"nan\")\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/Jupyterlab/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/miniconda3/envs/Jupyterlab/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Jupyterlab/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Jupyterlab/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Jupyterlab/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Jupyterlab/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o112.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 24) (luca executor driver): org.apache.spark.SparkException: Failed to execute user defined function(ALSModelParams$$Lambda$3108/0x000000084125b040: (int) => int)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1449)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: ALS only supports values in Integer range for columns ReviewerID and ProductID. Value null was not numeric.\n\tat org.apache.spark.ml.recommendation.ALSModelParams.$anonfun$checkedCast$1(ALS.scala:104)\n\tat org.apache.spark.ml.recommendation.ALSModelParams.$anonfun$checkedCast$1$adapted(ALS.scala:89)\n\t... 33 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDD.$anonfun$isEmpty$1(RDD.scala:1557)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.isEmpty(RDD.scala:1557)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:947)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:709)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(ALSModelParams$$Lambda$3108/0x000000084125b040: (int) => int)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1449)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: ALS only supports values in Integer range for columns ReviewerID and ProductID. Value null was not numeric.\n\tat org.apache.spark.ml.recommendation.ALSModelParams.$anonfun$checkedCast$1(ALS.scala:104)\n\tat org.apache.spark.ml.recommendation.ALSModelParams.$anonfun$checkedCast$1$adapted(ALS.scala:89)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"ReviewerID\", itemCol=\"ProductID\", ratingCol=\"Rating\",\n",
    "          coldStartStrategy=\"nan\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eadba03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "+----------+---------+------+--------------+\n",
      "\n",
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "|         0|     null|   1.0|     953424000|\n",
      "|         1|     null|   2.0|    1345507200|\n",
      "|         1|     null|   5.0|    1265760000|\n",
      "|         1|     null|   5.0|    1279670400|\n",
      "|         2|     null|   5.0|     962236800|\n",
      "|         3|     null|   4.0|    1380240000|\n",
      "|         3|     null|   5.0|    1389744000|\n",
      "|         4|     null|   5.0|    1036540800|\n",
      "|         4|     null|   5.0|    1384300800|\n",
      "|         6|     null|   3.0|    1293667200|\n",
      "|         6|     null|   5.0|     971049600|\n",
      "|         6|     null|   5.0|    1363046400|\n",
      "|         7|     null|   1.0|    1385942400|\n",
      "|         7|     null|   3.0|    1316217600|\n",
      "|        10|     null|   3.0|    1108512000|\n",
      "|        10|     null|   5.0|    1384992000|\n",
      "|        11|     null|   3.0|    1037836800|\n",
      "|        11|     null|   5.0|    1388707200|\n",
      "|        13|     null|   3.0|    1395014400|\n",
      "|        13|     null|   5.0|    1151366400|\n",
      "+----------+---------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ERROR: java.lang.IllegalArgumentException: \n",
    "ALS only supports values in Integer range for columns ReviewerID and ProductID.\n",
    "Value null was not numeric.\"\"\"\n",
    "\n",
    "training.filter(\"ReviewerID is NULL\").show()\n",
    "training.filter(\"ProductID is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99c5cb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "+----------+---------+------+--------------+\n",
      "\n",
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = training.dropna()\n",
    "training.filter(\"ReviewerID is NULL\").show()\n",
    "training.filter(\"ProductID is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da29cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "+----------+---------+------+--------------+\n",
      "\n",
      "+----------+---------+------+--------------+\n",
      "|ReviewerID|ProductID|Rating|unixReviewTime|\n",
      "+----------+---------+------+--------------+\n",
      "|         1|     null|   5.0|    1179619200|\n",
      "|         2|     null|   5.0|     955670400|\n",
      "|         6|     null|   5.0|    1373673600|\n",
      "|        10|     null|   1.0|    1193097600|\n",
      "|        10|     null|   3.0|    1013472000|\n",
      "|        13|     null|   5.0|    1059177600|\n",
      "|        17|     null|   4.0|    1096156800|\n",
      "|        22|     null|   5.0|    1372982400|\n",
      "|        24|     null|   4.0|    1046822400|\n",
      "|        33|     null|   5.0|    1350345600|\n",
      "|        34|     null|   5.0|    1357689600|\n",
      "|        35|     null|   5.0|    1216944000|\n",
      "|        39|     null|   5.0|    1336521600|\n",
      "|        45|     null|   5.0|    1366761600|\n",
      "|        48|     null|   5.0|    1397520000|\n",
      "|        57|     null|   3.0|    1165968000|\n",
      "|        59|     null|   5.0|    1399334400|\n",
      "|        70|     null|   4.0|    1303257600|\n",
      "|        75|     null|   5.0|    1362355200|\n",
      "|        78|     null|   5.0|    1196294400|\n",
      "+----------+---------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.filter(\"ReviewerID is NULL\").show()\n",
    "test.filter(\"ProductID is NULL\").show()\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6921d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(userCol=\"ReviewerID\", itemCol=\"ProductID\", ratingCol=\"Rating\",\n",
    "          coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcf499f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder()\\\n",
    ".addGrid(als.rank, [12, 13, 14])\\\n",
    ".addGrid(als.maxIter, [18, 19, 20])\\\n",
    ".addGrid(als.regParam, [.17, .18, .19])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c92586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Rating\", \n",
    "                                predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f516ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=als, estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d057a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvs.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e635d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a89530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b13b5b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 1.4868219383808512\n",
      "**BEST MODEL**\n",
      "Rank:  14\n",
      "MaxIter:  20\n",
      "RegParam:  0.19\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE = \" + str(rmse))\n",
    "print(\"**BEST MODEL**\")\n",
    "print(\"Rank: \", best_model.rank)\n",
    "print(\"MaxIter: \", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"RegParam: \", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6583523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ReviewerID</th><th>ProductID</th><th>Rating</th><th>unixReviewTime</th><th>prediction</th></tr>\n",
       "<tr><td>2</td><td>792838742</td><td>3.0</td><td>970444800</td><td>1.4603155</td></tr>\n",
       "<tr><td>2</td><td>1608838137</td><td>5.0</td><td>1378425600</td><td>3.4005356</td></tr>\n",
       "<tr><td>4</td><td>784010218</td><td>5.0</td><td>1402790400</td><td>4.750594</td></tr>\n",
       "<tr><td>6</td><td>790734680</td><td>5.0</td><td>1395705600</td><td>4.199687</td></tr>\n",
       "<tr><td>7</td><td>792837746</td><td>4.0</td><td>1360713600</td><td>3.7072403</td></tr>\n",
       "<tr><td>7</td><td>767839145</td><td>5.0</td><td>1369526400</td><td>4.0521116</td></tr>\n",
       "<tr><td>7</td><td>1573303593</td><td>5.0</td><td>1385337600</td><td>3.705644</td></tr>\n",
       "<tr><td>9</td><td>1573622990</td><td>4.0</td><td>1370563200</td><td>3.0953841</td></tr>\n",
       "<tr><td>11</td><td>790732203</td><td>5.0</td><td>1391558400</td><td>4.1523786</td></tr>\n",
       "<tr><td>14</td><td>790730987</td><td>5.0</td><td>1376870400</td><td>4.7076497</td></tr>\n",
       "<tr><td>18</td><td>1574927663</td><td>4.0</td><td>1355616000</td><td>2.1018414</td></tr>\n",
       "<tr><td>18</td><td>783231032</td><td>5.0</td><td>1325721600</td><td>2.0435755</td></tr>\n",
       "<tr><td>20</td><td>767002652</td><td>1.0</td><td>1395705600</td><td>4.212433</td></tr>\n",
       "<tr><td>21</td><td>1404983082</td><td>3.0</td><td>1146873600</td><td>3.900799</td></tr>\n",
       "<tr><td>21</td><td>784010188</td><td>5.0</td><td>1225411200</td><td>4.3659678</td></tr>\n",
       "<tr><td>22</td><td>767812158</td><td>3.0</td><td>1255392000</td><td>3.9790063</td></tr>\n",
       "<tr><td>22</td><td>1616842490</td><td>5.0</td><td>1362528000</td><td>4.0796385</td></tr>\n",
       "<tr><td>24</td><td>790733420</td><td>5.0</td><td>1376611200</td><td>3.589113</td></tr>\n",
       "<tr><td>25</td><td>310263662</td><td>4.0</td><td>1079568000</td><td>3.3384247</td></tr>\n",
       "<tr><td>25</td><td>783225199</td><td>5.0</td><td>1183334400</td><td>3.6926184</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+----------+------+--------------+----------+\n",
       "|ReviewerID| ProductID|Rating|unixReviewTime|prediction|\n",
       "+----------+----------+------+--------------+----------+\n",
       "|         2| 792838742|   3.0|     970444800| 1.4603155|\n",
       "|         2|1608838137|   5.0|    1378425600| 3.4005356|\n",
       "|         4| 784010218|   5.0|    1402790400|  4.750594|\n",
       "|         6| 790734680|   5.0|    1395705600|  4.199687|\n",
       "|         7| 792837746|   4.0|    1360713600| 3.7072403|\n",
       "|         7|1573303593|   5.0|    1385337600|  3.705644|\n",
       "|         7| 767839145|   5.0|    1369526400| 4.0521116|\n",
       "|         9|1573622990|   4.0|    1370563200| 3.0953841|\n",
       "|        11| 790732203|   5.0|    1391558400| 4.1523786|\n",
       "|        14| 790730987|   5.0|    1376870400| 4.7076497|\n",
       "|        18|1574927663|   4.0|    1355616000| 2.1018414|\n",
       "|        18| 783231032|   5.0|    1325721600| 2.0435755|\n",
       "|        20| 767002652|   1.0|    1395705600|  4.212433|\n",
       "|        21|1404983082|   3.0|    1146873600|  3.900799|\n",
       "|        21| 784010188|   5.0|    1225411200| 4.3659678|\n",
       "|        22| 767812158|   3.0|    1255392000| 3.9790063|\n",
       "|        22|1616842490|   5.0|    1362528000| 4.0796385|\n",
       "|        24| 790733420|   5.0|    1376611200|  3.589113|\n",
       "|        25| 310263662|   4.0|    1079568000| 3.3384247|\n",
       "|        25| 783225199|   5.0|    1183334400| 3.6926184|\n",
       "+----------+----------+------+--------------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions.sort(\"ReviewerID\", \"Rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36279dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
